{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 1: Introduction to Feature Engineering\n",
        "==========================================\n",
        "\n",
        "Key Learning Objectives:\n",
        "1. Understand what feature engineering is and why it matters\n",
        "2. Learn to identify opportunities for feature engineering\n",
        "3. See the impact of feature engineering on model performance"
      ],
      "metadata": {
        "id": "UHnrAjNxxztm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy import stats\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "xh5y_KP3x6At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('loan_applications.csv')\n",
        "\n",
        "\n",
        "# Display basic dataset information\n",
        "print(\"Dataset Overview:\")\n",
        "print(f\"Number of samples: {len(df)}\")\n",
        "print(f\"Original features: {df.columns.tolist()}\")\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "Ym3Y2rkfx9Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Creating Simple Features\n",
        "-------------------------\n",
        "Let's create our first engineered feature: Debt-to-Income Ratio"
      ],
      "metadata": {
        "id": "inMgZaHvy40E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create debt-to-income ratio feature\n",
        "df['debt_to_income'] = df['monthly_payment'] * 12 / df['income']\n",
        "\n",
        "# Handle any potential infinite or NaN values\n",
        "df['debt_to_income'] = df['debt_to_income'].replace([np.inf, -np.inf], np.nan)\n",
        "df['debt_to_income'] = df['debt_to_income'].fillna(df['debt_to_income'].mean())"
      ],
      "metadata": {
        "id": "wRFCfpAcyQae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the new feature\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(data=df, x='debt_to_income', hue='default', multiple=\"dodge\", bins=30)\n",
        "plt.title('Debt-to-Income Ratio Distribution\\nby Default Status')\n",
        "plt.xlabel('Debt-to-Income Ratio')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=df, x='default', y='debt_to_income')\n",
        "plt.title('Debt-to-Income Ratio vs Default')\n",
        "plt.xlabel('Default Status')\n",
        "plt.ylabel('Debt-to-Income Ratio')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PuI_3hx4yR0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_features(X, y, feature_set_name=\"\"):\n",
        "    # Handle missing values\n",
        "    X = X.fillna(X.mean())\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train and evaluate model\n",
        "    model = LogisticRegression(random_state=42)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"\\nModel Performance with {feature_set_name}:\")\n",
        "    print(f\"ROC-AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "    return auc_score"
      ],
      "metadata": {
        "id": "cwSDbSDayVW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART 3: Evaluating Feature Impact\n",
        "--------------------------\n",
        "Compare model performance with and without engineered features."
      ],
      "metadata": {
        "id": "JTXnzopDzBv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original features\n",
        "original_features = ['loan_amount', 'income']\n",
        "original_score = evaluate_features(\n",
        "    df[original_features],\n",
        "    df['default'],\n",
        "    \"Original Features\"\n",
        ")"
      ],
      "metadata": {
        "id": "rhpl0kP7zHK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original + engineered features\n",
        "engineered_features = original_features + ['debt_to_income']\n",
        "engineered_score = evaluate_features(\n",
        "    df[engineered_features],\n",
        "    df['default'],\n",
        "    \"Original + Engineered Features\"\n",
        ")"
      ],
      "metadata": {
        "id": "GOjHlsJ3zJNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize performance comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(['Original Features', 'With Engineered Features'],\n",
        "        [original_score, engineered_score])\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylabel('ROC-AUC Score')\n",
        "plt.ylim(0.5, 1.0)  # AUC score range from 0.5 to 1.0\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O1o6-_q7zKxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Lecture 2: Understanding the Dataset\n",
        "==================================\n",
        "\n",
        "Key Learning Objectives:\n",
        "1. Learn how to analyze features for engineering opportunities\n",
        "2. Understand relationships between features\n",
        "3. Identify patterns that suggest useful feature transformations"
      ],
      "metadata": {
        "id": "lbAYRfdpzYJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n"
      ],
      "metadata": {
        "id": "nRPUjiCmtNsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9wC2owOstV12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NXDlydnItW4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kXVvA2q60qfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorize features\n",
        "numerical_features = []\n",
        "categorical_features = []\n",
        "temporal_features = []\n",
        "target = None"
      ],
      "metadata": {
        "id": "bA77hBtItUHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze distributions of numerical features\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(numerical_features[:6], 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.histplot(df[feature], kde=True)\n",
        "    plt.title(f'{feature} Distribution')\n",
        "    # Add skewness information\n",
        "    skewness = stats.skew(df[feature].dropna())\n",
        "    plt.text(0.7, 0.9, f'Skewness: {skewness:.2f}',\n",
        "             transform=plt.gca().transAxes)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pqlp2-t39VXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create box plots for numerical features by default status\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(numerical_features[:6], 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    sns.boxplot(x='default', y=feature, data=df)\n",
        "    plt.title(f'{feature} by Default Status')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "npgqBtuk9i-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze categorical features\n",
        "for feature in categorical_features:\n",
        "    print(f\"\\n{feature} value counts:\")\n",
        "    print(df[feature].value_counts())\n",
        "\n",
        "    # Calculate default rate by category\n",
        "    default_rates = df.groupby(feature)['default'].mean()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    default_rates.plot(kind='bar')\n",
        "    plt.title(f'Default Rate by {feature}')\n",
        "    plt.ylabel('Default Rate')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pMFfRP3R9lYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create correlation matrix for numerical features\n",
        "# TODO: Corr matrix\n",
        "correlation_matrix = None\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Feature Correlations')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Examine potential interactions\n",
        "plt.figure(figsize=(15, 5))"
      ],
      "metadata": {
        "id": "VB5QT7hE9otM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Income vs Credit Score by Default\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(df[df['default']==0]['income'],\n",
        "           df[df['default']==0]['credit_score'],\n",
        "           alpha=0.5, label='Non-Default')\n",
        "plt.scatter(df[df['default']==1]['income'],\n",
        "           df[df['default']==1]['credit_score'],\n",
        "           alpha=0.5, label='Default')\n",
        "plt.xlabel('Income')\n",
        "plt.ylabel('Credit Score')\n",
        "plt.title('Income vs Credit Score')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "erisBLYU9q4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loan Amount vs Income by Education\n",
        "plt.subplot(1, 3, 2)\n",
        "for education in df['education'].unique():\n",
        "    mask = df['education'] == education\n",
        "    plt.scatter(df[mask]['income'],\n",
        "               df[mask]['loan_amount'],\n",
        "               alpha=0.5, label=education)\n",
        "plt.xlabel('Income')\n",
        "plt.ylabel('Loan Amount')\n",
        "plt.title('Loan Amount vs Income by Education')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V-66aA3P9vgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert application_date to datetime\n",
        "df['application_date'] = None\n",
        "\n",
        "# Analyze temporal patterns\n",
        "df['month'] = None\n",
        "df['year'] = None"
      ],
      "metadata": {
        "id": "7qgi_PYy9xmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot default rate over time\n",
        "monthly_default_rate = df.groupby(['year', 'month'])['default'].mean()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "monthly_default_rate.plot()\n",
        "plt.title('Default Rate Over Time')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Default Rate')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fyfbngvp90Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering Opportunities Identified:\n",
        "----------------------------------------\n",
        "1. Numerical Features:\n",
        "   - Income and loan_amount show right skewness → Log transformation\n",
        "   - Credit score has outliers → Binning or normalization\n",
        "   - Employment length has missing values → Imputation needed\n",
        "\n",
        "2. Categorical Features:\n",
        "   - Education shows clear relationship with default → Ordinal encoding\n",
        "   - City has high cardinality → Need dimensionality reduction\n",
        "   - Occupation has meaningful groups → Potential for grouping\n",
        "\n",
        "3. Temporal Features:\n",
        "   - Monthly patterns in default rate → Create cyclical features\n",
        "   - Application recency might matter → Create time-based features\n",
        "\n",
        "4. Potential Interactions:\n",
        "   - Income and education → Create income-education interaction\n",
        "   - Loan amount and income → Create loan-to-income ratio\n",
        "   - Credit score and income → Create risk segments"
      ],
      "metadata": {
        "id": "2S0cq-Jt94VB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 3: Data Cleaning and Missing Value Handling\n",
        "====================================================\n",
        "\n",
        "\n",
        "Key Learning Objectives:\n",
        "1. Learn how to handle missing values appropriately\n",
        "2. Understand different imputation strategies\n",
        "3. Implement data cleaning techniques\n",
        "4. Validate cleaning results"
      ],
      "metadata": {
        "id": "wm01D6I--N5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "3PUgLyEd-WQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('loan_applications.csv')"
      ],
      "metadata": {
        "id": "cqNY_Vcy-Yhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze missing values\n"
      ],
      "metadata": {
        "id": "NuPCn_c7-ayz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing Values Analysis:\")\n",
        "print(\"-----------------------\")\n",
        "for column, percentage in missing_percentages[missing_percentages > 0].items():\n",
        "    print(f\"{column}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "GZCkbj8k-b9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize missing value patterns\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
        "plt.title('Missing Value Patterns')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A32tGZSJ-d-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the dataframe for cleaning\n"
      ],
      "metadata": {
        "id": "y8UMojpg-hMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Handle employment_length missing values\n",
        "# Strategy: Use median for different education levels\n",
        "print(\"\\nMedian employment length by education level:\")\n",
        "print(None)"
      ],
      "metadata": {
        "id": "oeDrjUIs-kVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d3Emr_5Z-niY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Handle income missing values\n",
        "# Strategy: Use a more sophisticated imputation based on education and occupation\n",
        "\n",
        "# REMOVE missing values - rows\n",
        "print(\"\\nMedian income by education and occupation:\")\n",
        "print(df.groupby(['education', 'occupation'])['income'].median().head())\n",
        "\n"
      ],
      "metadata": {
        "id": "CkppQZyn-vxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Handle credit_score missing values\n",
        "# Strategy: Use a simple imputer with median strategy\n"
      ],
      "metadata": {
        "id": "FVKTdEP7-x9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Handle existing_loans missing values\n",
        "# Strategy: Fill with 0 (assume no existing loans if not specified)\n"
      ],
      "metadata": {
        "id": "npwIhlgO-zm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check remaining missing values\n",
        "remaining_missing = df_cleaned.isnull().sum()\n",
        "print(\"\\nRemaining missing values after imputation:\")\n",
        "print(remaining_missing[remaining_missing > 0])"
      ],
      "metadata": {
        "id": "Y7-Q4TKe-3bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate imputation results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "\n",
        "# Compare original vs imputed distributions for key features\n",
        "features_to_validate = ['employment_length', 'income', 'credit_score', 'existing_loans']\n",
        "for i, feature in enumerate(features_to_validate):\n",
        "    ax = axes[i // 2, i % 2]\n",
        "\n",
        "    # Plot original distribution\n",
        "    sns.kdeplot(data=df[feature].dropna(), ax=ax, label='Original', alpha=0.5)\n",
        "    # Plot imputed distribution\n",
        "    sns.kdeplot(data=df_cleaned[feature], ax=ax, label='After Imputation', alpha=0.5)\n",
        "\n",
        "    ax.set_title(f'{feature} Distribution Comparison')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "59h8ZfCp-4dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lesson 4 Outlier detection IQR method"
      ],
      "metadata": {
        "id": "7mheM-JO_SqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1200/1*0MPDTLn8KoLApoFvI0P2vQ.png)"
      ],
      "metadata": {
        "id": "2e9oP159DmRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_and_remove_outliers_iqr(df, column):\n",
        "    \"\"\"\n",
        "    Analyze, visualize, and remove outliers from a dataframe column using the IQR method.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): Input dataframe\n",
        "    column (str): Name of the column to remove outliers from\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: Dataframe with outliers removed/capped\n",
        "    \"\"\"\n",
        "    # Create a copy of the dataframe\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Calculate Q1, Q3, and IQR\n",
        "    Q1 = None\n",
        "    Q3 = None\n",
        "    IQR =  None\n",
        "\n",
        "    # Calculate bounds\n",
        "    lower_bound = None\n",
        "    upper_bound = None\n",
        "\n",
        "    # Print statistics before removal\n",
        "    print(f\"\\n=== Analysis for {column} ===\")\n",
        "    print(\"\\nBefore outlier removal:\")\n",
        "    print(f\"Count: {df[column].count()}\")\n",
        "    print(f\"Mean: {df[column].mean():.2f}\")\n",
        "    print(f\"Median: {df[column].median():.2f}\")\n",
        "    print(f\"Std: {df[column].std():.2f}\")\n",
        "    print(f\"Min: {df[column].min():.2f}\")\n",
        "    print(f\"Max: {df[column].max():.2f}\")\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = df[\n",
        "        (df[column] < lower_bound) |\n",
        "        (df[column] > upper_bound)\n",
        "    ]\n",
        "    print(f\"\\nNumber of outliers detected: {len(outliers)}\")\n",
        "    print(f\"Outliers percentage: {(len(outliers)/len(df))*100:.2f}%\")\n",
        "\n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Before removal boxplot\n",
        "    plt.subplot(131)\n",
        "    sns.boxplot(y=df[column])\n",
        "    plt.title('Before Removal\\nBoxplot')\n",
        "\n",
        "    # Before removal distribution\n",
        "    plt.subplot(132)\n",
        "    sns.histplot(df[column], kde=True)\n",
        "    plt.axvline(lower_bound, color='r', linestyle='--', label='Lower bound')\n",
        "    plt.axvline(upper_bound, color='r', linestyle='--', label='Upper bound')\n",
        "    plt.title('Before Removal\\nDistribution')\n",
        "    plt.legend()\n",
        "\n",
        "    # Cap the outliers at the bounds\n",
        "    df_clean.loc[df_clean[column] > upper_bound, column] = upper_bound\n",
        "    df_clean.loc[df_clean[column] < lower_bound, column] = lower_bound\n",
        "\n",
        "    # After removal distribution\n",
        "    plt.subplot(133)\n",
        "    sns.histplot(df_clean[column], kde=True)\n",
        "    plt.title('After Removal\\nDistribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics after removal\n",
        "    print(\"\\nAfter outlier removal:\")\n",
        "    print(f\"Count: {df_clean[column].count()}\")\n",
        "    print(f\"Mean: {df_clean[column].mean():.2f}\")\n",
        "    print(f\"Median: {df_clean[column].median():.2f}\")\n",
        "    print(f\"Std: {df_clean[column].std():.2f}\")\n",
        "    print(f\"Min: {df_clean[column].min():.2f}\")\n",
        "    print(f\"Max: {df_clean[column].max():.2f}\")\n",
        "\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "NkJWvnDb_DUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "numerical_features = ['income', 'loan_amount', 'monthly_payment']\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "for feature in numerical_features:\n",
        "    df_cleaned = analyze_and_remove_outliers_iqr(df_cleaned, feature)"
      ],
      "metadata": {
        "id": "k4gZGOGS_PDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Lecture 5: Feature Transformations\n",
        "=========================================\n",
        "\n",
        "\n",
        "Key Learning Objectives:\n",
        "1. Learn when and why to transform features\n",
        "2. Understand different scaling techniques\n",
        "3. Apply basic mathematical transformations\n",
        "4. Validate transformation results"
      ],
      "metadata": {
        "id": "VzpNwcby_eVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler,\n",
        "    MinMaxScaler,\n",
        "    RobustScaler\n",
        ")"
      ],
      "metadata": {
        "id": "uM2OwB56_hJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('loan_applications.csv')"
      ],
      "metadata": {
        "id": "gdJTPka__i-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART 1: Scaling Transformations\n",
        "--------------------------\n",
        "Apply different scaling techniques to numerical features."
      ],
      "metadata": {
        "id": "vIl9dKEr_mWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical features for scaling\n",
        "numerical_features = ['income', 'loan_amount', 'monthly_payment',\n",
        "                     'credit_score', 'employment_length']\n",
        "df_transformed = df.copy()"
      ],
      "metadata": {
        "id": "kCsUVHlk_lfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. StandardScaler (z-score normalization)\n",
        "\n",
        "![](https://i0.wp.com/cdn-images-1.medium.com/max/370/1*Nlgc_wq2b-VfdawWX9MLWA.png?ssl=1)"
      ],
      "metadata": {
        "id": "qDOkRPTH3fQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = None"
      ],
      "metadata": {
        "id": "3b2gET18_qOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. MinMaxScaler (to 0-1 range)\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRfalbkrBP7E2nthrr667MhjxO1SOsVDLOTXw&s)\n"
      ],
      "metadata": {
        "id": "3iG4GPH03n7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minmax = None"
      ],
      "metadata": {
        "id": "s7pGX3Me_sNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. RobustScaler (using quartiles)\n",
        "\n",
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20230428205714/for4.png)"
      ],
      "metadata": {
        "id": "1b5Gxcui31NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "robust = None"
      ],
      "metadata": {
        "id": "Mb5p3umb_tTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART 2: Mathematical Transformations\n",
        "-------------------------------\n",
        "Apply basic mathematical transformations to handle skewness\n",
        "and non-linear relationships."
      ],
      "metadata": {
        "id": "XlOJ3Az9_2xC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Logarithmic transformation\n",
        "\n",
        "![](https://theailearner.com/wp-content/uploads/2019/01/log-1.png)"
      ],
      "metadata": {
        "id": "ghAc-FsO4KLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in ['income', 'loan_amount', 'monthly_payment']:\n",
        "    df_transformed[f'{feature}_log'] = np.log1p(df[feature])"
      ],
      "metadata": {
        "id": "KUepA88u_5bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Square root transformation"
      ],
      "metadata": {
        "id": "af4__YaZ4cSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in ['credit_score', 'employment_length']:\n",
        "    df_transformed[f'{feature}_sqrt'] = np.sqrt(df[feature])"
      ],
      "metadata": {
        "id": "s1PY0NAZ_7PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Power transformation\n"
      ],
      "metadata": {
        "id": "nzU5snkI4ek3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in ['income', 'loan_amount']:\n",
        "    df_transformed[f'{feature}_squared'] = np.square(df[feature])"
      ],
      "metadata": {
        "id": "X5n2IM10_8nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART 3: Basic Ratio Features\n",
        "-----------------------\n",
        "Create simple ratio features from numerical variables."
      ],
      "metadata": {
        "id": "eIpNVqcWAC_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Basic financial ratios\n"
      ],
      "metadata": {
        "id": "5nTsOWis4lGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed['loan_to_income'] = None\n",
        "df_transformed['payment_to_income'] = None\n",
        "df_transformed['payment_to_loan'] = None"
      ],
      "metadata": {
        "id": "Knn9wU6AACS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize ratio distributions by default status\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Ratio Features by Default Status')\n",
        "\n",
        "ratios = ['loan_to_income', 'payment_to_income', 'payment_to_loan']\n",
        "for i, ratio in enumerate(ratios):\n",
        "    sns.boxplot(x='default', y=ratio, data=df_transformed, ax=axes[i])\n",
        "    axes[i].set_title(ratio)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OinZN9cFAHYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 6: Feature Transformation\n",
        "==================================\n",
        "\n",
        "\n",
        "Key Learning Objectives:\n",
        "1. Learn advanced transformation techniques\n",
        "2. Understand when to use each transformation\n",
        "3. Handle complex financial relationships\n",
        "4. Validate transformation effectiveness"
      ],
      "metadata": {
        "id": "8CxPDPsw4xYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import (\n",
        "    PowerTransformer,\n",
        "    QuantileTransformer,\n",
        "    FunctionTransformer\n",
        ")"
      ],
      "metadata": {
        "id": "in9M0VZA5bTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('loan_applications.csv')"
      ],
      "metadata": {
        "id": "xjeReALB5dnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## PART 1: Advanced Distribution Transformations\n",
        "---------------------------------------\n",
        "Apply transformations to handle complex distributions in\n",
        "financial data."
      ],
      "metadata": {
        "id": "XV8c7icn5xmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features for transformation\n",
        "financial_features = ['income', 'loan_amount', 'monthly_payment', 'credit_score']\n",
        "df_transformed = df.copy()"
      ],
      "metadata": {
        "id": "9MtsX5Nk5w24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Yeo-Johnson transformation (handles negative values)\n",
        "pt_yj = PowerTransformer(method='yeo-johnson')\n",
        "transformed_yj = pt_yj.fit_transform(df[financial_features])\n",
        "\n",
        "for i, feature in enumerate(financial_features):\n",
        "    df_transformed[f'{feature}_yeojohnson'] = transformed_yj[:, i]"
      ],
      "metadata": {
        "id": "PXENPTic51tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Quantile transformation (uniform distribution)\n",
        "qt_uniform = QuantileTransformer(output_distribution='uniform')\n",
        "transformed_uniform = qt_uniform.fit_transform(df[financial_features])\n",
        "\n",
        "for i, feature in enumerate(financial_features):\n",
        "    df_transformed[f'{feature}_uniform'] = transformed_uniform[:, i]\n"
      ],
      "metadata": {
        "id": "BpmrJqgw53BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Quantile transformation (normal distribution)\n",
        "qt_normal = QuantileTransformer(output_distribution='normal')\n",
        "transformed_normal = qt_normal.fit_transform(df[financial_features])\n",
        "\n",
        "for i, feature in enumerate(financial_features):\n",
        "    df_transformed[f'{feature}_normal'] = transformed_normal[:, i]"
      ],
      "metadata": {
        "id": "tTbIXalE55Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize transformations\n",
        "fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n",
        "fig.suptitle('Distribution Transformations Comparison')\n",
        "\n",
        "for i, feature in enumerate(financial_features):\n",
        "    # Original distribution\n",
        "    sns.histplot(df[feature], ax=axes[i, 0])\n",
        "    axes[i, 0].set_title(f'Original {feature}')\n",
        "\n",
        "    # Yeo-Johnson\n",
        "    sns.histplot(df_transformed[f'{feature}_yeojohnson'], ax=axes[i, 1])\n",
        "    axes[i, 1].set_title(f'Yeo-Johnson {feature}')\n",
        "\n",
        "    # Uniform\n",
        "    sns.histplot(df_transformed[f'{feature}_uniform'], ax=axes[i, 2])\n",
        "    axes[i, 2].set_title(f'Uniform {feature}')\n",
        "\n",
        "    # Normal\n",
        "    sns.histplot(df_transformed[f'{feature}_normal'], ax=axes[i, 3])\n",
        "    axes[i, 3].set_title(f'Normal {feature}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zryFx0SC57Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2: Domain-Specific (Financial Domain) Transformations\n",
        "----------------------------------\n",
        "Apply transformations specific to financial metrics."
      ],
      "metadata": {
        "id": "7mVSVEI658q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Risk-adjusted features\n",
        "def risk_adjust(x, risk_factor):\n",
        "    return x * (1 - risk_factor)\n",
        "\n",
        "risk_features = ['income', 'loan_amount']\n",
        "risk_factor = None\n",
        "\n",
        "for feature in risk_features:\n",
        "    df_transformed[f'{feature}_risk_adj'] = None"
      ],
      "metadata": {
        "id": "wbXsaC4P6EXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Smoothed temporal features\n",
        "def exponential_smooth(x, alpha=0.3):\n",
        "    return pd.Series(x).ewm(alpha=alpha).mean()\n",
        "\n",
        "temporal_features = ['monthly_payment']\n",
        "for feature in temporal_features:\n",
        "    df_transformed[f'{feature}_smoothed'] = None"
      ],
      "metadata": {
        "id": "svzNhLs66FwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Bounded transformations\n",
        "def sigmoid_transform(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh_transform(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Apply bounded transformations to normalized features\n",
        "for feature in financial_features:\n",
        "    normalized = None\n",
        "    df_transformed[f'{feature}_sigmoid'] = None\n",
        "    df_transformed[f'{feature}_tanh'] = None"
      ],
      "metadata": {
        "id": "ACQ_1dz-AgHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 7: Categorical Encoding\n",
        "================================\n",
        "\n",
        "Key Learning Objectives:\n",
        "1. Understand different encoding techniques\n",
        "2. Learn when to use each encoding method\n",
        "3. Handle high-cardinality categories\n",
        "4. Implement target-based encoding"
      ],
      "metadata": {
        "id": "Y1WeGDVS6cUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/v2/resize:fit:1400/1*ggtP4a5YaRx6l09KQaYOnw.png)"
      ],
      "metadata": {
        "id": "3I0SyEDbEHqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import (\n",
        "    LabelEncoder,\n",
        "    OneHotEncoder,\n",
        "    OrdinalEncoder,\n",
        "    TargetEncoder\n",
        ")"
      ],
      "metadata": {
        "id": "iw3YjZnJ6gdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('loan_applications.csv')"
      ],
      "metadata": {
        "id": "Qa7IFEx-67Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical columns\n",
        "categorical_features = ['education', 'occupation', 'city', 'gender']\n",
        "df_encoded = df.copy()"
      ],
      "metadata": {
        "id": "VawRtuZY6_qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Label Encoding (for ordinal categories)\n",
        "education_order = ['High School', 'Bachelor', 'Master', 'PhD']\n",
        "le = None"
      ],
      "metadata": {
        "id": "NyK-wFVK7BEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded[['education','education_label']]"
      ],
      "metadata": {
        "id": "AmHD0g_B7C3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the OneHotEncoder\n",
        "ohe = None\n",
        "\n",
        "# Define nominal features\n",
        "nominal_features = ['occupation', 'city']\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_nominal = None\n",
        "\n",
        "# Get feature names\n",
        "encoded_feature_names = ohe.get_feature_names_out(nominal_features)\n",
        "\n",
        "# Create a new dataframe with encoded features\n",
        "df_encoded = pd.DataFrame(\n",
        "    encoded_nominal,\n",
        "    columns=encoded_feature_names,\n",
        "    index=df.index\n",
        ")\n",
        "\n",
        "# If you want to add these columns to your original dataframe:\n",
        "df_new = pd.concat([df.drop(columns=nominal_features), df_encoded], axis=1)"
      ],
      "metadata": {
        "id": "8AiyMZgx7Oqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new"
      ],
      "metadata": {
        "id": "9u29i7gB7nc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Binary Encoding (for gender)\n",
        "df_encoded['gender_binary'] = None"
      ],
      "metadata": {
        "id": "Z_J-0-Z17rEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## PART 2: Handling High Cardinality\n",
        "---------------------------\n",
        "Deal with categorical variables that have many unique values."
      ],
      "metadata": {
        "id": "A-mCI5b77-PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Frequency-based encoding\n",
        "for feature in ['occupation', 'city']:\n",
        "    value_counts = df[feature].value_counts()\n",
        "    print(f'{feature}_freq')\n",
        "    df_encoded[f'{feature}_freq'] = df[feature].map(value_counts)"
      ],
      "metadata": {
        "id": "dua8zb7U8BeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded['city_freq']"
      ],
      "metadata": {
        "id": "8-rdd8aG8ISf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Top-K encoding with \"Other\" category\n",
        "def top_k_encoding(series, k=10):\n",
        "    return None\n",
        "\n",
        "df_encoded['city_top_k'] = top_k_encoding(df['city'])\n",
        "df_encoded['occupation_top_k'] = top_k_encoding(df['occupation'])"
      ],
      "metadata": {
        "id": "m9gTYTQX8LCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded['city_top_k']"
      ],
      "metadata": {
        "id": "0fVgAyk28NLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture 8: Feature Interactions\n",
        "================================\n",
        "\n",
        "Key Learning Objectives:\n",
        "1. Understand different types of feature interactions\n",
        "2. Create meaningful financial interaction features\n",
        "3. Combine categorical and numerical features\n",
        "4. Evaluate interaction effectiveness"
      ],
      "metadata": {
        "id": "5Lk3k-1O8v2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "\n",
        "# Load the encoded dataset\n",
        "df = pd.read_csv('loan_applications.csv')"
      ],
      "metadata": {
        "id": "0j1mslZt8pOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical features for interactions\n",
        "numerical_features = ['income', 'loan_amount', 'monthly_payment',\n",
        "                     'credit_score', 'employment_length']\n",
        "df_interactions = df.copy()"
      ],
      "metadata": {
        "id": "JImFn3_C8zNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Basic Financial Ratios\n",
        "df_interactions['debt_service_ratio'] = df['monthly_payment'] / df['income']\n",
        "df_interactions['loan_to_income'] = df['loan_amount'] / df['income']\n",
        "df_interactions['payment_to_loan'] = df['monthly_payment'] / df['loan_amount']"
      ],
      "metadata": {
        "id": "LAIUhSBi80df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Polynomial Interactions\n",
        "poly = None\n",
        "poly_features = ['loan_amount']\n",
        "poly_transformed = None\n",
        "poly_names = None"
      ],
      "metadata": {
        "id": "sdO3xC_681qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_names"
      ],
      "metadata": {
        "id": "uHE4HWUB880L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, name in enumerate(poly_names):\n",
        "    if i >= len(poly_features):  # Skip original features\n",
        "        df_interactions[f'poly_{name}'] = poly_transformed[:, i]\n",
        "\n",
        "# 3. Risk-Weighted Features\n",
        "df_interactions['risk_weighted_income'] = None\n",
        "df_interactions['risk_weighted_loan'] =  None"
      ],
      "metadata": {
        "id": "O6wIaxQ89AbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2: Categorical-Numerical Interactions\n",
        "------------------------------------\n",
        "Create interactions between categorical and numerical features."
      ],
      "metadata": {
        "id": "XzBQVMlC9Iod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Education-Income Interaction\n",
        "education_income_mean = df.groupby('education')['income'].transform('mean')\n",
        "df_interactions['relative_income'] = df['income'] / education_income_mean"
      ],
      "metadata": {
        "id": "JaQ14Xjq9MGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Occupation-Loan Interaction\n",
        "occupation_loan_mean = df.groupby('occupation')['loan_amount'].transform('mean')\n",
        "df_interactions['relative_loan'] = df['loan_amount'] / occupation_loan_mean"
      ],
      "metadata": {
        "id": "XtbxVKpI9Np6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6dnmklTj9O08"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}